---
title        : "Ames Housing in R"
subtitle     : smart-R
author       : Floris Padt
date-format  : "DD-MM-YYYY"
date         : '2 Jun 2023'
date-modified: last-modified
description  : "JADS Foundation - cohort May 2023"
abstract     : |
               | In the Ames Housing dataset, which is commonly used for predicting housing prices, there are several 
               | features that can significantly influence the sales price of a house. The importance of these features 
               | can vary depending on the specific dataset and the machine learning algorithm used for analysis. 
               | However, based on general observations and common practices, 
               | the following features are often considered as strong predictors of housing prices: 
               |
               | 1. Overall Quality: The overall quality of a house, usually measured on a scale from 1 to 10, is a crucial factor affecting its sales price. Higher-quality homes tend to command higher prices.  
               | 2. Above Ground Living Area: The size of the above ground living area, typically measured in square feet, is a strong indicator of a house's value. Larger houses generally have higher prices.  
               | 3. Number of Bedrooms: The number of bedrooms in a house is an important factor for many buyers. Houses with more bedrooms are typically priced higher.  
               | 4. Number of Bathrooms: Similarly, the number of bathrooms in a house plays a significant role in determining its value. More bathrooms often lead to higher prices.  
               | 5. Lot Size: The size of the lot on which a house is situated can influence its price. Larger lots are generally associated with higher prices, especially in desirable locations.  
               | 6. Neighborhood: The neighborhood in which a house is located can have a significant impact on its value.  
title-block-banner: "#FFFFFF"
format:
  html:
    anchor-sections: true
    embed-resources: true
    smooth-scroll: true
    theme: cosmo
    fontcolor: "#370037"
    toc: true
    toc-location: left
    toc-title: Summary
    toc-depth: 3
    code-fold: true
    code-copy: true
    highlight: tango
    link-external-icon: true
    link-external-newwindow: true
# filters: 
#   - custom-callout.lua
css: smart-r.css
editor: visual
execute:
  echo: true
  eval: true
---

```{r}
#| label: init
#| eval:  FALSE
#| echo: false

#install.packages("renv")

install.packages('rmarkdown') # needed for Quarto
install.packages('tufte')     
install.packages('curl')
install.packages('kableExtra')
install.packages('DT')
install.packages('data.table')
install.packages('magrittr')
install.packages('gridExtra')

install.packages('broom')
install.packages('psych')
install.packages('tidyverse')
install.packages('corrplot')
install.packages('ggplot2')
install.packages('caret')
install.packages('Metrics')
install.packages('e1071')
install.packages('glmnet')
```

```{r}
#| label:   setup
#| include: false
#| echo: false

invisible({
#  library(psych)      # package to describe your data
  library(tidyverse)  # easy way to subset your data
  library(corrplot)   # to draw correlation plots
#  library(ggplot2)    # to plot graphs | already in tidyverse
  library(caret)      # to run machine learning models
  library(Metrics)    # to calculate RMSE
  library(e1071)      # for statistical analyses
  library(glmnet)     # for statistical analyses

  library(knitr)      # to knit Rmd to md
  library(kableExtra) # additional formatting for tables
  library(DT)         # D3 tables
  library(data.table) # data manipulation https://rdatatable.gitlab.io/data.table/
  library(gridExtra)  # for grid.arrange
  library(broom)      # tidy model output
#  library(magrittr)   # piping | already in tidyverse
})

options(scipen=999) # turn off scientific notation

# general functions

# table of missing values per variable
f_kbl_with_NA <-
  function(dt){
    dt[, lapply(.SD, function(x) sum(is.na(x)))] %>%
    melt.data.table(measure.vars = names(.))     %>%
    .[value > 0] %>%
    setorder(-value) %>%
    kbl(
      align = "l"
    )
  }

# global variable.names
github_ames <- 
  "https://raw.githubusercontent.com/jads-nl/discover-projects/main/ames-housing/"

```

# Ames Housing in R

```{r}
#| label: read_data
#| eval:  TRUE
#| echo: true

uri <- paste0(github_ames, "AmesHousing.csv")

df =  read.csv(uri) # data.frame
dt <- fread(uri)    # data.table
```

## Introduction

The *Ames Housing* dataset contains information from the Ames Assessor's Office used in computing assessed values for individual residential properties sold in Ames, Iowa \[IA\] from 2006 to 2010.\
The dataset has 2,930 observations with 82 variables  (23 nominal, 23 ordinal, 14 discrete, and 20 continuous). 
For a complete description of all included variables, please look at: <https://rdrr.io/cran/AmesHousing/man/ames_raw.html>.

[syllabus](https://jads-foundation-syllabus.netlify.app/) [Discover Projects](https://github.com/jads-nl/discover-projects)

## Exercise 1:

Familiarize yourself with the data.

Provide a table with descriptive statistics for all included variables and check:

-   Classes of each of the variables (e.g. factors or continuous variables).

-   Descriptive/summary statistics for all continuous variables (e.g. mean, SD, range) and factor variables (e.g. frequencies).

-   Explore missing values: `sapply(df, function(x) sum(is.na(x)))`

### Data Set

```{r}
#| label: show hear using DT 
dt %>%
setcolorder(c("Order", "SalePrice")) %>%
DT::datatable(
  caption = "Table 1: Ames Housing dataset",
  class = "compact stripe",
  rownames = FALSE,
  filter = 'top',
  extensions = c('FixedColumns'),
  options = list(
    scrollX      = TRUE,
    fixedColumns = list(leftColumns = 2)
    )
 )  %>% 
  formatCurrency("SalePrice", '\U0024', digits = 0) %>%
  formatStyle(
    'SalePrice',
    color              = "#003700",
    fontWeight         = "bold",
    backgroundColor    = '#FFFFF0',     
    backgroundSize     = '100% 60%',
    backgroundRepeat   = 'no-repeat',
    backgroundPosition = 'center'
  ) %>%
  formatStyle(
    'Order',  
    color              = '#C0C0C0', 
    backgroundColor    = '#FFFFF0'
  )
```

::: callout-note
-   Use the base-R function `str` (no package needed)
-   Use the `describe` function (from the psych-package) for continuous variables
-   Use the `table` function (base-R) for factor variables.
:::

```{r}
#| label: 'check the structure'

# To check the structure of the data, you can use the "str"-command:
# str(dt)

# create a table with the type of the data
dt_str <-
  dt[, lapply(.SD, typeof)]               %>% 
  melt.data.table(
    measure.vars    = names(.),
    variable.factor = FALSE)              %>%
  setorder(value, variable )             

# display a summery per type
dt_str %>%
  .[, .(count = .N), by = value] %>%
  DT::datatable(
    caption = "Table 2: Data structure summary",
    class = "compact stripe",
    rownames = FALSE,
    options = list(
      dom = "t"
    )
  ) %>%
  formatStyle(
    "value",
    color              = "#370037",
    backgroundColor    = "#FFFFF0",
    fontWeight         = "bold"
  )

# display structure/type of the data  
dt_str %>%
  DT::datatable(
    caption = "Table 3: Data structure and types",
    class = "compact stripe",
    rownames = FALSE,
    filter = "top"
  ) %>%
    formatStyle(
      "variable",
      color              = "#370037",
      backgroundColor    = "#FFFFF0",
      fontWeight         = "bold"
    )

dt_chr <- dt_str[value == "character", variable]
dt_int <- dt_str[value == "integer", variable]
```

All factor variables now have the 'character' class.\
The following code helps to convert each character variable into a factor variable:

```{r}
#| label: 'convert character to factor'

df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
# str(df)

# convert character variables to factor variables
chr2fct <- function(x){
    if(is.character(x)) 
      as.factor(x) 
    else 
      x
  }

# convert character variables to factor variables
# keep the integers
dt[, names(dt):= lapply(.SD, chr2fct)]

# display the factors and levels
str(dt[, ..dt_chr])
```

### Explore missing values

Create a table with the number of missing values per variable.\

```{r}
#| label: Explore missing values

# sapply(df, function(x) sum(is.na(x))) 

# table of missing values per variable
f_kbl_with_NA(dt) %>%
  kable_styling(
    full_width      = FALSE, 
    position        = "left",
    htmltable_class = "lighttable-hover lighttable-condensed lightable-striped"
    ) 
```

### Descriptive statistics

Create a table with descriptive statistics for all included variables.\
For continuous variables, you can use the `describe` function (from the psych-package).\
For factor variables, you can use the `table` function (base-R).

```{r}
#| label: describe numeric and integer variables

dt[, psych::describe(.SD), .SDcols = dt_int] %>%
  as.data.table(keep.rownames = "cont_vars") %>%
  DT::datatable(
    caption  = "Table 4: Describe numerics",
    class    = "stripe",
    rownames = FALSE,
    filter   = "top",
    extensions = c('FixedColumns'),
    options = list(
      scrollX      = TRUE,
      fixedColumns = list(leftColumns = 1)
    )
  ) %>%
  formatStyle(
    "cont_vars",
    color              = "#370037",
    backgroundColor    = "#FFFFF0",
    fontWeight         = "bold"
  )

```


```{r}
#| label: describe factor variables
#| eval:  false

my_cnt <-
  function(x){
    data.table(col = x) %>%
    .[, .(cnt = .N), by = col] 
}

dt<=dtb 

dt[, (names(dt)) := lapply(.SD, as.factor), .SDcols = sapply(dt, is.character)]

# Reshape the data.table into long format
cols <- sapply(dt, is.factor) %>% .[.==TRUE]
dt6  <- dt[, ..cols]

dt_long <- melt(dt6, measure.vars = names(dt6), variable.name = "Column")

# Create bar chart for each column
ggplot(dt_long, aes(x = fct_infreq(value))) +
  geom_bar() +
  facet_wrap(~Column, scales = "free_x") +
  labs(x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

tst<- lapply(dt5, my_cnt) 

  dt[, .()]
  ggplot( mapping = aes(x = f, y = cnt)) +
    geom_col() +
    coord_flip() +
    facet_wrap(facets = vars(c), scales = "free")

temp <- 
  df %>%
  purrr::keep(is.factor)

for (i in 1:ncol(temp)) {
  print(names(temp[i]))
  print(table(temp[, i]))
}
```

## Exercise 2:

There a several missing values in the dataset, which need to be tackled before we can proceed with the rest of the analysis.

There are many ways to impute missing values, but for now, impute missing values for numeric variables with the median, and impute missings in all factor variables with the label "100".

### Imputation of missing values for numeric variables

```{r}
#| label: Median imputation for continuous variables

# impute NA with median for all numeric variables
dt[, (dt_int) := lapply(.SD, function(x){
  ifelse(is.na(x), median(x, na.rm=T), x)}), .SDcols = dt_int]

# table of missing values per variable
f_kbl_with_NA(dt)

df <-
  lapply(df, function(x) {
    ### Impute median for all missing numeric values
    if(is.numeric(x)) ifelse(is.na(x), median(x, na.rm=T), x) else x
  }
  ) %>%
  data.frame()
```

### Imputation of missing values for factor variables

```{r}
#| label: 100 imputation for factor variables

# generate a vector with variable names for all factor variables
factor_variables <- 
  df              %>%
  keep(is.factor) %>% 
  names

# impute missing values for factor variables
df<-
  lapply(df,function(x) {
    if(is.factor(x)) ifelse(is.na(x),"100",x) else x
  }) %>%
  data.frame()

# 100 imputation for factor variables
dt[, (dt_chr) := lapply(.SD, function(x) {
  ifelse(is.na(x), "100", as.character(x))
}), .SDcols = dt_chr]

# convert factor variables back to factor variables 
# (imputation turned them into character variables)
df[factor_variables] <- lapply(df[factor_variables], factor)
dt[, (dt_chr) := lapply(.SD, as.factor), .SDcols = dt_chr]
     
```

### check for missing values

```{r}
#| label: Check missing values

# sapply(df, function(x) sum(is.na(x)))

# table of missing values per variable
f_kbl_with_NA(dt)
```

### check for blank values 

#### before imputation

```{r}
#| label: Check blank values before

# table of blank values per variable
dt[, lapply(.SD, function(x) sum(trimws(x) == '', na.rm = TRUE))] %>%
    melt.data.table(measure.vars = names(.))     %>%
    .[value > 0] %>%
    setorder(-value) %>%
    kbl(
      align = "l"
    )
```

#### impute '100'

```{r}
# 100 imputation for factor variables
dt[, (dt_chr) := lapply(.SD, function(x) {
  ifelse(x == '', "100", as.character(x))
}), .SDcols = dt_chr]

# convert factor variables back to factor variables 
dt[, (dt_chr) := lapply(.SD, as.factor), .SDcols = dt_chr]
```

#### after imputation

```{r}
#| label: Check blank values after

# table of blank values per variable
dt[, lapply(.SD, function(x) sum(trimws(x) == '', na.rm = TRUE))] %>%
    melt.data.table(measure.vars = names(.))     %>%
    .[value > 0] %>%
    setorder(-value) %>%
    kbl(
      align = "l"
    )
```



### list the variables and their values

```{r}
#| label: List the variables with values
#| eval:  true

dtVV <- 
  dt[, ..dt_chr] %>%
  melt.data.table(measure.vars = dt_chr) %>%
  unique() 

dtVV %>%
  DT::datatable(
    caption    = "Table 4a: Variable Values",
    class      = "stripe",
    rownames   = FALSE,
    filter     = "top",
    extensions = c("FixedColumns"),
    options = list(
      scrollX      = TRUE,
      fixedColumns = list(leftColumns = 1)
    )
  ) %>%
  formatStyle(
    "variable",
    color              = "#370037",
    backgroundColor    = "#FFFFF0",
    fontWeight         = "bold"
  )
```

## Exercise 4 - 

Explore the outcome variable (SalePrice) and how it correlates to other features

The variable "SalePrice" refers to the price at which a property was sold and hence is the variable of interest for our prediction model ("Y" or dependent variable).

Please explore Y in terms of:

a. Conduct descriptive/summary statistics on the Y variable (mean, median, SD, range)
b. Investigate how neighborhood (categorical) and grand living area (continuous) relate to the Y variable; use, e.g., bar charts, scatter plots, boxplots
c. Visualize the distribution of the Y variable. What do you observe?
d. Assess the distribution of SalePrice in the previous exercise. What do you observe? Log-transform the outcome variable. What does it mean for the performance of the prediction model?
e. Assess grand living area ('Gr Liv Area') for all houses in previous exercise. What do you observe? Remove outliers. What does it mean for the applicability of the prediction model?
f. Draw scatter plots between Y and all numerical features 
g. Draw correlation plots to see all correlations between Y and the independent (continuous) variables (Hint: calculate Pearson correlation coefficient)

-   Visualize the distribution of Y (e.g. use base-R "hist" or "ggplot" from the "ggplot2"-package)

-   Visualize the distribution of Y by looking at various subgroups\
    (e.g. create boxplot or scatterplot using the "ggplot2"-package).

-   Look at differences between neighborhoods.

-   Look at differences between housing style.

-   Draw a correlation plot to see all correlations between Y and the independent (numeric) variables.

::: callout-note
For visualization, ggplot is frequently used as it provides a flexible way to draw a lot of different graphs.

`ggplot` contains two basic elements:

1.  The initiation command:\
    `ggplot(DATASET, aes(x=XVAR, y=YVAR, group=XVAR))`\
    This draws a blank ggplot. Even though the x and y are specified, there are no points or lines in it.

2.  Add the respective geom of interest (for this exercise you'll need:\
    `+ geom_point()` (for scatterplot) or\
    `+ geom_boxplot()`

The full code to write a scatter plot would then be:

`ggplot(DATASET, aes(x=XVAR, y=YVAR)) + geom_point()`
:::

::: callout-note
To draw a correlation plot. Please use the "corrplot"-package.\
Using this package, one can construct a correlation plot in two steps:

1.  Use "cor" to calculate correlation between all combinations of numeric variables\
    select numeric variables by using: `df %>% keep(is.numeric)`

2.  Plot the calculated correlation by using the `corrplot` -function
:::

### Descriptive/summary statistics

```{r}
# Descriptive/summary statistics (e.g. mean, SDs, range)

dt$SalePrice                   %>%
  psych::describe()            %>%
  t()                          %>%
  as.data.table(
    keep.rownames = "stat")    %>%
  .[, .(stat, 
        SalesPrice = X1)]      %>%
  kbl(
    digits      = 0,
    caption     = "Table 5: Descriptive statistics for Sales Price",
    format.args = list(big.mark = ","),
    align       = 'l'
  ) %>%
  kable_styling(
    full_width      = FALSE, 
    position        = "left",
    htmltable_class = "lighttable-hover lighttable-condensed lightable-striped") 

```

### Visualize the distribution of Y

```{r}

# Visualize the distribution of Y 
# (e.g. use base-R "hist" or "ggplot" from the "ggplot2"-package)
hist(dt$SalePrice)

ggplot(data = dt, aes(SalePrice)) + 
  geom_histogram(fill = "#005100", color = "#FFFFF0", bins = 18) + 
  # scale_x_continuous(limits = c(0,600000), expand = c(0, 0)) +
  # scale_y_continuous(limits = c(0,650)   , expand = c(0, 0)) +
  labs(title = "Histogram of Sale Price") +
  ylab(label = "Count") + 
  xlab(label = "Sale Price") +
  # theme_classic() +
  theme(
    axis.title.x = element_text(
      colour = "#370037", size = 11.5, face = "bold"), 
    axis.title.y = element_text(
      colour = "#370037", size = 11.5, face = "bold"),
    plot.title = element_text(
      colour = "#370037", size = 18  , face = "bold", hjust = 0)
  ) 
```

### Visualize Y by Lot Area and Neighbourhood

```{r}

# Visualize the distribution of Y by looking at various subgroups 
# (e.g. create boxplot or scatterplot using the "ggplot2"-package)

# Scatterplot
p1 <-
  ggplot(data = dt, aes(x = `Lot Area`, y = SalePrice)) + 
    geom_point(size = .7, color = "#005100") +
    scale_x_continuous(limits = c(0, 50000) , expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 600000), expand = c(0, 0)) +
    labs(title = "Scatterplot Sale Price by Lot Area") +
    ylab(label = "Sale Price") + 
    xlab(label = "Lot area") +
    # theme_classic() +
    theme(
      axis.title.x = element_text(
        colour = "#370037", size = 11.5, face = "bold"), 
      axis.title.y = element_text(
        colour = "#370037", size = 11.5, face = "bold"), 
      plot.title = element_text(
        colour = "#370037", size = 18  , face = "bold", hjust = 0))

# Side-by-side plots, only 1 
grid.arrange(p1, nrow = 1)


# Boxplot
dt[, avgSP := mean(SalePrice), by = Neighborhood]       %>%
  .[, Neighborhood := fct_reorder(Neighborhood, avgSP)] %>%
  .[, avgSP := NULL]                                    %>%
  ggplot(aes(x = Neighborhood, y = SalePrice)) + 
    geom_boxplot(color = "#005100", fill = "#FFFFF0") +
    labs(title = "Boxplot Sale Price by Neighbourhood") +
    ylab(label = "Sale Price") + 
    xlab(label = "Neighbourhood") +
    # theme_classic() +
    theme(
      axis.title.x = element_text(
        colour = "#370037", size = 11.5, face = "bold"), 
      axis.title.y = element_text(
        colour = "#370037", size = 11.5, face = "bold"), 
      plot.title = element_text(
        colour = "#370037", size = 18  , face = "bold", hjust = 0),
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    )
```

### Visualize Y by House Style

::: callout-note
Box-plots are sorted by the mean of the dependent variable (SalePrice).  The mean of the dependent variable is calculated for each level of the independent variable (House Style).\
The levels of the independent variable are reordered based on the mean of the dependent variable.
:::

```{r}
#|label: Look at differences between housing style
dt[, avgHS := mean(SalePrice), by = `House Style`]        %>%
  .[, `House Style` := fct_reorder(`House Style`, avgHS)] %>%
  .[, avgHS := NULL]                                    %>%  
  ggplot(aes(x = `House Style`, y = SalePrice)) + 
    geom_boxplot(color = "#005100", fill = "#FFFFF0") +
    labs(title = "Boxplot Sale Price by House Style") +
    ylab(label = "Sale Price") + 
    xlab(label = "House Style") +
    # theme_classic() +
    theme(
      axis.title.x = element_text(
        colour = "#370037", size = 11.5, face = "bold"), 
      axis.title.y = element_text(
        colour = "#370037", size = 11.5, face = "bold"), 
      plot.title = element_text(
        colour = "#370037", size = 18  , face = "bold", hjust = 0),
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    )
     

```

### Correlation plot

```{r}
#| label: 'Correlation plot '
#| 
# corr_df <- 
#   df               %>% 
#   keep(is.numeric) %>% 
#   cor

 corr_dt <-
  dt[, ..dt_int]   %>% 
  cor(
    use    = "everything", 
    method = "pearson"
  ) 

corrplot(
  corr          = corr_dt, 
  type          = "upper",
  title         = "Correlation between all numeric variables in the dataset", 
  diag          = FALSE,
  order         = 'hclust',
  hclust.method = 'median',
  addrect       = 3,
  number.font   = 2, 
  tl.cex        = 0.50,
  mar           = c(0, 0, 1, 0)
)

corr_dt[, "SalePrice"]      %>%
  as.data.table(
    keep.rownames = "var",
    check.names   = FALSE
  )                         %>%
  setnames(".", "corr")     %>%
  setorder(-corr)           %>%
  kbl()
  

```

## Exercise 5 - Estimate a Linear Regression, a LASSO and a kNN model

Now that we have a better feeling of the information in the data set and we took care of the missing values, we can start by running some (additional) simple machine learning models.\

We will use the "caret"-package for this exercise. Split the data randomly into a train set (70%) and test set (30%)

### train-test split

```{r}
#| label: 'split data in 70%-30%'
#| 
set.seed(1234)

# use the caret::createDataPartition function to split the data
Index <- 
  createDataPartition(dt$Order, p = 0.7, list = FALSE)

train <- dt[ Index, ]
test  <- dt[-Index, ]

```

### Cross-validation

Next we need to specify how we want to perform the cross-validation (i.e. the optimization of the model on the train set). To this extend we need to set the method of CV, the number of folds and the numer of times we want to repeat the process. We will use the "repeatedcv" method, with 10 folds and 3 repeats.

```{r}
#| label: Cross-validation

# Cross-validation strategy from the caret package
ctrl <- 
  trainControl(
    method  = "repeatedcv",
    number  = 5,   # ten folds
    repeats = 3)   # repeated three times
```

### Modeling

a.  Estimate a Linear Regression model
b.  Estimate a LASSO model
c.  Estimate a kNN model

#### Linear Regression model

```{r fig.width = 20, fig.height = 30}
#| label: Linear Regression model
#| eval:  true
#| warning: false

# Scatterplot with smoother lm

copy(dt[, ..dt_int]) %>%
  melt.data.table(
    id.vars      = c("Order", "SalePrice")
  ) %>%
  ggplot(
    aes(x = value     , y = SalePrice)) + 
  geom_point(size = .7, color = "#005100") +
  geom_smooth(
    method = "lm",
    se     = FALSE, 
    color  = "#0000FF",
    lwd    = 2 ) +
  facet_wrap(
    ncol   = 4, 
    facets = ~ variable, 
    scales = "free") +
  # coord_cartesian(
  #   xlim = c(0,  50000),
  #   ylim = c(0, 600000)) +
  labs(title = "Scatterplot Sale Price by Variable") +
  ylab(label = "Sale Price") + 
  # xlab(label = "Lot area") +
  # theme_classic() +
  theme(
    axis.title.x = element_text(
      colour = "#370037", size = 11.5, face = "bold"), 
    axis.title.y = element_text(
      colour = "#370037", size = 11.5, face = "bold"), 
    plot.title = element_text(
      colour = "#370037", size = 18  , face = "bold", hjust = 0))

# Side-by-side plots
# grid.arrange(p2, nrow = 1)

```

Calculate how well the model explains the variance in the data (R2).

```{r}
#| label: Linear Regression
#| eval:  true

# Fit the linear regression model on the training data
model <- lm(SalePrice ~ ., data = train)

# View the summary of the model
sum_mod <- summary(model) 
paste(
  "Multiple R-squared:", round(sum_mod$r.squared    , 3), 	
  "Adjusted R-squared:", round(sum_mod$adj.r.squared, 3)
)

# Extract the coefficients and their standard errors
coefficients <- coef(model)

# Extract the p-values for each coefficient
p_values <- summary(model)$coefficients[, "Pr(>|t|)"]

# Coefficients: (6 not defined because of singularities)
setdiff(names(coefficients), names(p_values))

# create table with the coefficients and their importance measures
data.table(
  Variable    = names(p_values),
  Coefficient = coefficients[names(p_values)],
  P_Value     = p_values,
  Importance  = abs(coefficients[names(p_values)]) / sum(abs(coefficients[names(p_values)]))
) %>%
.[P_Value < 0.05]     %>%
setorder(-P_Value)    %>%
DT::datatable(
  caption    = "Table 6: Linear Regression model",
  class      = "compact stripe",
  rownames   = FALSE,
  filter     = 'top',
  extensions = c('FixedColumns'),
  options = list(
    scrollX      = TRUE,
    fixedColumns = list(leftColumns = 1)
    )
 )  %>% 
  formatStyle(
    'Variable',
    color              = "#003700",
    fontWeight         = "bold",
    backgroundColor    = '#FFFFF0'     
  ) 
```

> What does it mean when in a linear regression model you have singularities and how to solve this?

::: {.callout-tip collapse="true"}
A singularity in a linear regression model means that one or more of the independent variables can be expressed as a linear combination of the other independent variables.

This is a problem because it means that the model cannot distinguish between the effects of the variables that are linearly dependent.

To address singularities caused by multicollinearity, you can take the following steps:

Identify the variables causing multicollinearity: Look for high pairwise correlations or examine variance inflation factors (VIF) to identify the variables that contribute to multicollinearity.

Resolve multicollinearity:

Remove one or more of the highly correlated variables. Combine correlated variables to create new composite variables. Use dimensionality reduction techniques like principal component analysis (PCA). Assess the impact: Re-estimate the model after resolving multicollinearity and examine the changes in coefficients, standard errors, and significance levels.

`r tufte::quote_footer('--- chatGPT')`
:::

> Can I use AIC to determine which variables I need to use in my linear regression model?

::: {.callout-tip collapse="true"}
Yes, you can use the Akaike Information Criterion (AIC) to determine which variables to include in your linear regression model. \> The AIC is a metric that balances the goodness of fit of a model with its complexity, penalizing models with more parameters.

The general idea is to compare the AIC values of different models with different sets of variables and select the model with the \> lowest AIC as the preferred model.

We initially fit a model using all potential variables. Then, we iterate over each variable and fit models without each variable, calculating the AIC for each reduced model. The variables with the lowest AIC values are considered the most informative and are selected for the final model.

`r tufte::quote_footer('--- chatGPT')`
:::

```{r}
#| label: AIC
#| eval:  false

# set hyperparameter k
k <- 10

# Use the lm model generated 
initial_model <- model

# Calculate AIC for the initial model
initial_aic <- AIC(initial_model)

# Initialize a list to store the AIC values
aic_values <- list()

# train_df        <- as.data.frame(train)
train_dt <- 
  copy(train) %>%
  setNames(gsub(" ", "_", names(.)))

# names(train_dft <- gsub(" ", "_", names(train_dt))

# Iterate over each variable to evaluate its contribution to the model
for (var in names(train_dt)) {
  # Skip the dependent variable
  if (var == "SalePrice")
    next
  
  # Fit a model without the current variable
  reduced_model <- lm(formula(paste("SalePrice ~ . -", var)), data = train_dt)
  
  # Calculate AIC for the reduced model
  aic <- AIC(reduced_model)
  
  # Store the AIC value in the list
  aic_values[[var]] <- aic
}

# Sort the AIC values in ascending order
sorted_aic <- sort(unlist(aic_values))

# Identify the variables with the lowest AIC values
selected_vars <- names(sorted_aic)[1:k]

# Build the final model using the selected variables
final_model <- 
  lm(SalePrice ~ ., data = train_dt[, c("SalePrice", selected_vars), with = FALSE])

```



#### Lasso model

```{r}
#| label: LASSO
#| eval:  true

lambda <- 10^seq(-3, 3, length = 100)

lassoFit <- 
  train(
    SalePrice ~ ., 
    data       = train, 
    method     = "glmnet", 
    trControl  = ctrl, 
    preProcess = c("center", "scale"),
    tuneGrid   = expand.grid(alpha = 1, lambda = lambda))

lassoFit               # to obtain summary of the model
varImp(lassoFit)       # to see most important parameters
plot(varImp(lassoFit)) # to plot most important parameters
```

#### kNN model

```{r}
#| label: KNN

## Run kNN
knnFit <- 
  train(
    SalePrice ~ ., 
    data       = train, 
    method     = "knn", 
    trControl  = ctrl, 
    preProcess = c("center", "scale")
  )

knnFit               # to obtain summary of the model
plot(knnFit)
varImp(knnFit)       # to see most important parameters
plot(varImp(knnFit)) # to plot most important parameters
```

## Exercise 6 - Assess which model performs best

### Evaluation

The performance metric for the prediction model should be the Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sale price. This makes it the Root-Mean-Squared-Log-Error (RMSLE). By plotting a histogram of the sale price you will understand why the logarithm is recommended.

```{r}
#| label: 'predict using linear regression'
#| eval: false
 
# Make predictions on the test data
predictions <- predict(model, newdata = test)

# Calculate evaluation metrics (e.g., RMSE)
rmse <- caret::RMSE(predictions, test$SalePrice)
```

```{r}
#| label: 'compare Lasso versus KNN on RMSE'
#| eval: false
#| warnings: false

# LASSO
pred_lassoFit <- 
  predict(lassoFit, newdata = test)

lasso_rmse <- 
  rmse(
    actual    = test$SalePrice,
    predicted = pred_lassoFit
  ) %>% 
  round(3)

# KNN
pred_knn <- 
  predict(knnFit, newdata = test)

knn_rmse <- 
  rmse(
    actual    = test$SalePrice,
    predicted = pred_knn
  ) %>%
  round(3)

data.table(
  Model = c("Lasso"   , "KNN"),
  RMSE  = c(lasso_rmse, knn_rmse)
) %T>%
  setorder(RMSE)                  %>%
  .[, .(Rank= 1:.N, Model, RMSE)] %>%
  kbl(
    caption = "Model performance",
    align = 'l', 
    centering = F
  ) %>%
  kable_styling(
    full_width      = FALSE, 
    position        = "left",
    htmltable_class = "lighttable-hover lighttable-condensed lightable-striped"
  ) %>%

## Appendix


```

```{r}
#| label: 'appendix'
#| 
data_description <- 
  paste0(github_ames, "data_description.txt") %>%
  readLines()
```

```{asis}
`r data_description`
```
